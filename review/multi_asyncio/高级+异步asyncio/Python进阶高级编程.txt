课程安排：
	------面向对象
	鸭子类型
	抽象基类
	MRO属性查找算法和super函数
	静态方法、类方法、实例方法
	数据封装和私有属性
	对象的自省机制
	上下文管理器
	contextlib实现上下文管理器
	mixin继承模式的应用

	-------元类
	property动态属性
	__getattr__ 、__getarrtibute__的区别
	属性描述符
	__new__ 和 __init__ 区别 （面试常用）
	元类实现orm

	-------多线程
	GIL和多线程
	线程通信-共享变量、Queue
	线程同步-Lock、RLock、Condition、 Semaphores
	线程池和源码分析-ThreadPoolExecutor
	多进程-multiprocessing
	进程间通信

	-------异步IO
	IO多路复用-select、poll、epoll
	select+回调+事件循环模式
	生成器进阶-send 、close 、throw和yield from
	async和await

	-----asyncio
	Future和Task
	aiohttp实现高并发抓取url
 	asyncio背后的selector
	协程同步和通信
	ThreadPoolExecutor + asyncio




-------------------------************************----------------------
			第一章介绍

			第二章 面向对象
 1...一切皆对象
python面向对象更彻底
 函数和类也是对象，属于python的一等公民：
  赋值给一个变量
  可以添加到集合对象中
  可以作为参数传递给函数
  可以当做函数的返回值

2....type 、class、object之间的关系

 type-->class-->object
 
 type(class)--->>"type"
 type(type) --->>"type"
 type(object)-->>"type"

所以一切皆对象，type类是所有顶层类的类型，即衍生所有的类，连自己都实例化为自己，自己都不放过，
type.__bases__  -->>"object"
class.__bases__  -->>"objece"
object.__bases__  -->> ()  为空
所有类的顶层基类为object类，都追溯到底都继承自object类

但是object的类型为"type", 而type的基类为"object", 所以他们互相指向了对方，形成一个循环
这就是python的一切皆对象


3...python中常见的内置类型

  3.1 对象的三个特征： 身份（使用id()查看，即在内存中的地址）、类型（type（））、值
  3.2 None(全局只有一个）
  3.3 数值类型 ：int 、float 、complex(复数）、bool
  3.4 迭代类型
  3.5 序列类型：list、 tuple 、str、 byte(bytearray、memoryview（二进制序列))、range、array
  3.6 映射（dict）
  3.7 集合： set、frozenset
  3.8 上下文管理类型（with）
  3.9 其他类型：（模块类型、class和实实例、函数类型、方法类型、代码类型、object对象、type类型、ellipsis类型、notimplemented类型）


-----------------------***********************----------------------
			第三章 魔法函数

1..Python中自带的双下划线开头，双下划线结尾的函数
 
   在任何对象中都可以调用魔法函数，不仅仅是在object对象中。
  魔法函数的调用都是隐式的，是可以改变Python的语法，就比如__getitem__，当对象不是迭代对象的时候，自动在类中产生迭代语句，即返回的语句，实现迭代效果；不仅仅是这个用法


2..Python的魔法函数：
  
 非数学运算：
  字符串表示：__repr__  , __str__
  集合序列相关： __len__ ,__getitem__ , __setitem__ , __delitem__ , __contains__
   迭代相关： __iter__ , __next__ , 
   可调用： __call__
   with上下文管理器： __enter__ , __exit__ 
  数值转换： __abs__ , __bool__ , __int__ , __float__ , __hash__ , __index__
  元类相关： __new__ , __init__, 
  属性相关： __getattr__ , __setattr__ , __getattribute__ , __setattribute__ , __dir__
  属性描述符： __get__ , __set__ , __delete__
  协程： __await__ , __aiter__ , __anext__ , __aenter__ , __aexit__ 

 数学运算：
  很多很多
  一元运算符：
  二元运算符：
  算术运算符：
  反向算术运算符：
  增量赋值算术运算符：
  位运算符：
  反向位运算符：
  增量赋值运算符：

3..魔法函数的重要性

Python解释器..进行的内部优化

 len()函数的特殊性：
  在Python内部，原生的类型比如:list, set, dict 性能非常高，list, set, dict 其都是用c写的，在内部会有一个数据的统计，其调用len()的时候会走一个捷径，不用迭代，直接找到这个数字，返回就行了，，这是进行了性能的优化，比使用纯Python语言要高，其实Python还有很多性能优化的内部函数。
  
  包括for循环语句也是一样，当遇到for的时候，如果没有找到__iter__迭代器,会自动生寻找使用__getitem__函数，并小心翼翼的处理好异常。


----------------------*************************---------------

			第四章 深入类和对象

4.1 鸭子类型和多态

 鸭子类型才是Python的根本

   定义一个类的时候，只需要他们有共同的方法，不用像JAVA里那样，必须定义一个父类，继承一个指定类型
   所有的类都实现同一个方法，调用一个方法就可以实现每个类的方法

   Python的魔法函数其实就是一个鸭子类型，在任何一个类中都可以使用，不用需要什么类型。不用指定类型或者定义一个类
  定义一个类的时候，只需要他们有共同的方法

  
4.2 抽象基类（abc模块）
  
 @abc.abstractmethod
 
  类似于Java的接口，不能进行实例化

就是必须去实现一个类中的方法，就像是java的接口
就是为了使用鸭子类型，推荐不使用抽象基类，使用mixin多继承

我们在某些情况下希望判定某个对象的类型
使用 isinstance
 例子： class A:
	    pass
	class B(A):
	    pass

	b=B()
  	isinstance(b,A)

输出： TRUE
这就很像Java中的instanceof，用于判断类型的

# 我们需要强制某个子类必须实现某些方法

# 需要设计一个抽象基类，指定子类必须实现某些方法

4.3 isinstance 和type的区别

  is 表示的是两个的id相等
  == 表示的是两个的值相等
  isinstance 表示两个的类型
  type(A) 就表示A的id

所以在判断类型的时候，尽量使用isinstance,而不是type,因为容易产生误差

4.4 类变量和实例变量

 类变量和实例变量是两个不同的概念
 类变量是这个类共有的属性，不论实例化了多少个，类变量始终不变。
 除非 类.类变量进行赋值，才会改变、

4.5 类属性和实例属性以及查找顺序

 MRO  属性搜索算法
 python2.3 之前是  深度优先和广度优先算法
 python2.3 之后是 C3算法，自动进行运算 深度优先和广度优先算法

4.6 静态方法、类方法、实例方法
 
 实例方法 传递的是self，相应参数
 静态方法，传递相应参数，不需要其他的；前面加装饰器 @staticmethod
 类方法，传递的是 cls，相应参数; 前面加装饰器 @classmethod

 静态方法如果要返回的话，必须前面加 类名
 而类方法返回的话，是加cls
 这两句意思就是当类名改变的话，静态方法返回名也跟着改变，而类方法不用
class Date:
	pass
   # 静态方法
    @staticmethod
    def parse_from_string(date_str):
        year, month, day = tuple(date_str.split("-"))
        return Date(int(year), int(month), int(day))

    # 类方法
    @classmethod
    def from_string(cls, date_str):
        year, month, day = tuple(date_str.split("-"))
        return cls(int(year), int(month), int(day))

4.7、数据封装和私有属性

    # 如果前面加上了双下划线，变成私有属性，那么就不能进行直接访问
    # 'User' object has no attribute 'birthday'
    # print(user.birthday)
    # 但是其公共方法依然能够进行访问
    print(user.get_age())
    # 其实通过小技巧，一些机制依旧能够访问其私有属性，只是变得稍微麻烦一点:例如
    # _classname__private attribute
    # _类名__私有属性
    print(user._User__birthday)
因此，从语言来看并不安全，还是能通过其他机制来访问，并没有绝对的私有属性。


4.8 自省机制

 自省机制就是通过一定的机制查询到对象的内部结构
 使用 __dict__查询到对象的属性结构
 使用__dir__，同样能，但是比__dict__更加强大，列出的更多

4.9 super

 既然我们重写B的构造函数，为什么还要去调用super?
 这是为了在某些时候加载父类中的方法，而不用重新去定义，新的
 
 super并不一定是指 调用父类，在多继承的时候，就是MRO的顺序

 super 执行的顺序是什么：
    面对多继承的时候，其实就是 MRO 执行的顺序，即为 ：到底是深度优先还是广度优先

4.10 Mixin模式 （其实是为了实现多继承，但又不推荐使用多继承而产生的一种模式）

 mixin模式特点： 
  1、Mixin类功能单一
  2、不和基类关联，可以和任意基类组合，基类可以不和Mixin关联就行初始化成功
  3、在Mixin中不要使用super这种用法。

4.11 with语句

 try:
 except:
 else:
 finally:

except和else只会运行其中一个
如果四个语句中都有return语句，则会优先返回finally里面的

 
上下文管理器协议：

 with语句比try except更加好用，更加方便

 只要类的内部定义了__enter__和__exit__这两个魔法函数，就可以使用with语句、
class Sample:
    def __enter__(self):
        print("enter")
        # 获取资源
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # 释放资源
        print("exit")

    def do_something(self):
        print("doing_something")


with Sample() as sample:
    sample.do_something()

4.12 contextlib简化上下文管理器

  contextlib使用装饰器 @contextlib.contextmanager将一个函数直接变成上下文管理器，就可以使用with语句
 在函数其中必须有一个yield{}生成器
 在yield之前就是相当于 __enter__里的内容，yield之后的语句就相当于__exit__之后的内容。

import contextlib

@contextlib.contextmanager
def file_open(file_name):
    print("file open")
    yield {}
    print("file end")


with file_open("test.text"):
    print("file processing")



---------------------***************************----------------------

			第五章 自定义序列类

5.1 序列分类

容器序列： list, tuple, deque  (表示可以往里添加任何类型的数据）
扁平序列： str,  bytes, bytearray, array.array
可变序列： list, deque, bytearray, array
不可变序列：str, tuple, bytes

5.2 序列的abc继承关系

 from _collections_abc  import __all__

 Sequence 和 MutableSequence
 不可变序列    可变序列

 Sequence继承 Reversible和Collection，实现反转，长度、迭代、contains等功能。
 MutableSequence中， 重点的多了 __setitem__, __delitem__, __inser__等魔法方法，才能进行序列的变化。这些魔法方法构成了Python的协议

5.3 list中的 +, +=, extend的方法区别

a=[1,2]
b = a + [3,4]
>>[1,2,3,4]
a += (5,6)
>>[1,2,5,6]
a.extend(range(3))
>>[1,2,5,6,0,1,2]
a.append([7,8])
>>[1,2,5,6,0,1,2,[7,8]]

看出 :
     +  只能是两个List类型
     += 将任意序列 进行迭代，然后添加进去，其实内部方法还是extend
     extend 会先进行迭代，然后一个一个添加进list中
     append  会将总体添加到list中

5.4 实现可切片的对象
 
  切片是会返回一个新的列表，并不会改变初始列表 
  切片可以实现增删改查，功能非常强大

  实现自己切片功能，对于不可变序列 只要重写 __getitem__ 魔法函数就可以了，其他的 __reversed__，__len__，__iter__，__contains__等可以选择实现。


5.5 bisect维护已排序序列

   用来处理已排序的序列，用来维护已排序的序列，升序

   内部采用的是二分查找法，性能十分的高。

insert_list = []
bisect.insort(insert_list, 1)
bisect.insort(insert_list, 6)
bisect.insort(insert_list, 4)
bisect.insort(insert_list, 5)
bisect.insort(insert_list, 9)

print(insert_list)

# 打印出 数字5 位于insert_list的哪个位置
# 采用相同数字的左边插入法
# 意思就是当里面有与5相同的数字，就把他插入到相同数字的前面
print(bisect.bisect_left(insert_list, 5))


5.6 什么时候我们不该使用列表
  列表里面可以放任何东西，但有时候使用起来并不高效和顺手

  使用 数组：array ；双单： deque 
   这两个都比列表的性能高

import array

 # array, deque
 # 数组， 双单
 # array和list的一个重要区别，array只能存放指定的数据类型，而list可以存放任意类型
 
 # 具体存放的类型需要在官方文档上查看
my_array = array.array("i")  # 这里的 i 表示无符号的整数类型
my_array.append(1)
print(my_array)


5.7 列表推导式、生成器表达式、字典推导式、 集合推导式

  列表生成式（列表推导式）：通过一行代码来生成列表
  生成式的性能要远高于列表的语句。

odd_list = [i for i in range(21) if i % 2 == 1]


  生成器表达式：

odd_gen = (i for i in range(21) if i % 2 == 1)
odd_list = list(odd_gen)
print(type(odd_gen))
print(odd_list)

  字典推导式： 通过一行代码来生成字典

my_dict = {"xing1": 22, "xing2": 23, "xing3": 21}
reverse_dict = {value: key for key, value in my_dict.items()}
print(reverse_dict)

  集合推导式

my_set = {key for key, value in my_dict.items()}
print(type(my_set))
print(my_set)



-------------------*******************************---------------------

			第六章 深入set和dict

set 和dict 具有非常高的性能，内部使用C语言写的，最好能多使用

6.1 dict的abc继承关系

  dict 属于 mapping的类型
  from collections.abc import Mapping, MutableMapping
 a={}
 print(isinstance(a, MutableMapping))
>>> TRUE


6.2 dict里的方法
 # 清空dict
# a.clear()

# copy ，浅拷贝
# 浅拷贝只是复制了引用，当改变新的值的时候，原来的值也会跟着改变
new_dict = a.copy()

# 深拷贝
# 将值都复制了一份，改变当前的值并不会改变原来的值
import copy
new_dict1 = copy.deepcopy(a)

# fromkeys()方法
# 将可迭代的序列 变成一个字典结构
new_list = ["xing1", "xing2"]
new_dict2 = dict.fromkeys(new_dict, {"company": "qinghua"})

# get() 方法
# 得到需要的值，如果没有返回默认
value = new_dict2.get("xing", {})

# setdefault() 方法
# 返回需要的值，如果没有就添加进去
value3 = new_dict2.setdefault("xing2")
value4 = new_dict2.setdefault("xing3", "beida")

# update() 方法
# 传递一个键值对进去、或者 可迭代的序列...
new_dict2.update({"xing4": "hello"})
print(new_dict2)

# 传递一个iterable对象
new_dict2.update(xing5="chognqing", xing6="shenzhen")
new_dict2.update((("xign8","shagnhai"),))


6.3 dic的子类

 不建议继承list和dict
 因为他们内部使用C语言写的，当你重写父类方法的时候，可能并不会调用

 from collections import UserDict
 如果真的需要继承的话，可以继承UserDict

 dict 的子类: defaultdict
 from collections import defaultdict
 my_dict = defaultdict(dict)
 value = my_dict["xign"]

当没有这个key的时候，就会调用内置的 __missing__方法，返回一个{}


6.4 set和frozenset

# set集合
# frozenset  不可变集合
# 都是无序的，不重复的

s= set("abcdef")

s1 = {"a", "b", "c"}

s2 = set(["a","b","c"])

s3 = frozenset("abcedfdf")


# 向set添加数据
s.add(3)

s.update([5, 6])

# update()还可以将两个set连成一个集合
another_set = {"3", "z", "y"}
s.update(another_set)

# 集合的运算
# 返回s中有，another_set中没有的元素
# 即 s-another_set
re_set = s.difference(another_set)
print(re_set)

# 集合运算常用的有: | & -  (并集、交集、差集）
re_set1 = s - another_set
re_set2 = s & another_set
re_set3 = s | another_set



6.5  dict 和 set 的实现原理

 dict 查找的性能远远大于list
 在List中随着list的数据的增大，查找时间会增大
 在dict中查找元素不会随着dict的增大而增大


dict内部是一个哈希表，通过将key值进行 hash （即假如是“abc”，就将<< c+随机数>> 生成一个hash值放在哈希表里面，其实哈希表里面是一个数组，所以查询的时候就会直接找到key对应的哈希值，然后找到相应的数据。从而实现快速的查询，所以无论数据有多大都不影响查找的数据，根据数据偏移量直接找到对应的数据，不用遍历整个数组）

但是就出现一个问题，当存进去的值可能发生冲突，，所以出现冲突的时候，就需要重新进行哈希值的计算，重新安排一个位置给当前数据。比如还是那个“abc”，然后哈希值就变成<< bc+随机数>>了，如果还是冲突的话，就再往前面推一位，直到没有冲突为止。


1. dict的key或者set的值，都必须是可以hash的（列表就不行）
 不可变对象，都是可以hash的，str, frozenset, tuple, 自己实现的类__hash__

2. dict 的内存花销大，但是查询速度快，自定义的对象，或者python内部的对象都是用dict包装的 （为什么内存占用大呢，就是因为当存入的hash表，空余部分小于三分之一的时候，就会将这个表拷贝一份，然后申请一个更大的哈希表，这是为了防止存入的数据位置冲突，减小冲突的概率）

3. dict的存储顺序和元素添加顺序有关

4、 添加数据都有可能改变已有数据的顺序。
 （3、4这两条都还是因为hash存储的机制，冲突比较大。）



------------------*****************************--------------------------

		第七章 对象引用、可变性和垃圾回收

7.1 Python中的变量是什么

 # python 和Java中的变量不一样，Python的变量实质是一个指针，int , str,  便利贴

a = 1
a = "abc"

# 1、先有1，然后将a贴在了1上面
# 2、先 生成对象，然后贴便利贴

# 而Java是先声明一个变量，然后将值放在这个变量的盒子里面，与Python本质不同
# 比如  int a=1  就是先声明一个int变量a，然后将1放在a这个盒子里面


7.2  is 和 == 的区别

 == 其实内部是 __equals__方法, 比较的是值
 is 是 id() 方法，比较的是id值

但是出现 :
         a=[1,2,3,4]
         b=[1,2,3,4]
这种赋值的时候，a和b的id就不一样了
而 b=a 这种引用的时候，a和b的id就是一样了

****重要：：在Python中有一个 intern 机制，当出现 小整数 或者小字符串的时候， Python会 建立一个全局为 小整数的对象，下一次再用小整数的时候，就会指向这个全局为小整数的对象， 即他们的 id 值是相等的， 并不会再去重新生成一个，这是Python内部的优化机制
即：  a=1
      b=1

id(a) 和 id (b)  是相等的


7.3 del语句和垃圾回收

python中垃圾回收的算法是： 采用 引用计数

当一个变量的引用为0的时候，Python就会将其回收
del a 
del 语句就是删除引用，比如 a

a=1, 
b = a,
del a

b 还是指向了1，而 a 已经被删除了

其中，Python有一个魔法函数 __del__（）
  这个函数可以在对变量进行引用删除的时候做一些事情，比如实现一些功能啥的


7.4 list作为参数的一些坑

def addd(a,b):
    a += b
    return a


a = [1, 2]
b = [3, 4]
c = addd(a, b)

print(c)  # [1, 2, 3, 4]
print(a)  # [1, 2, 3, 4]
print(b)  # [3, 4]

为什么输出的a 变了呢，就是因为共用了addd()方法里的变量a

class Company:
    def __init__(self, name, staffs=[]):
        self.name = name
        self.staffs = staffs

    def add(self, staff_name):
        self.staffs.append(staff_name)

    def remove(self, staff_name):
        self.staffs.remove(staff_name)


if __name__ == "__main__":
    com1 = Company("com1", ["xing1", "xing2"])
    com1.add("xing3")
    com1.remove("xing1")
    print(com1.staffs)  # ['xing2', 'xing3']

    com2 = Company("com2")
    com2.add("xing4")
    print(com2.staffs)  # ['xing4']

    com3 = Company("com5")
    com3.add("xing5")
    print(com2.staffs)  # ['xing4', 'xing5']
    print(com3.staffs)  # ['xing4', 'xing5']



# 为什么输出的com2.staffs变了呢：
# 两个原因：
  # 1、def __init__(self, name, staffs=[]): 这里传入了一个空的列表，这是一个可变参数
  # 2、当没有指定传入的staffs参数时，使用默认的staffs，此时com2和com3共用了一个变量
# 所以造成输出的com2和com3值相同

# 因此，我们要少用list这种形式的参数，少传入 staffs=[]这种类型




------------------------*************************--------------------

			第八章 元类编程

8.1 property动态属性

  @property
    def age(self):
        return datetime.now().year - self.birthday.year

通过 user.age 直接获取属性值

    @age.setter
    def age(self, value):
        self._age = value

通过 user.age = 30  直接设置属性值

from datetime import date, datetime


class User:
    def __init__(self, name, birthday):
        self.name = name
        self.birthday = birthday
        self._age = 0

    @property
    def age(self):
        return datetime.now().year - self.birthday.year

    @age.setter
    def age(self, value):
        self._age = value


if __name__ == "__main__":
    user = User("xing", date(year=1995, month=8, day=9))
    print(user.age)

    user.age = 30
    print(user._age)


 8.2  __getattr__ 和__getattribute__魔法函数

 __getattr__ 这是魔法函数是在查找不到属性的时候调用
 __getattribute__ 是无条件调用，无论有没有找到属性，都会调用这个魔法函数，因此，最好不要覆盖此方法，只有在写框架的时候才会使用到


class User:
    def __init__(self, info):
        self.info = info

    def __getattr__(self, item):
        return self.info[item]   # "xing"

    def __getattribute__(self, item):
        return "hello"           # "hello


if __name__ == "__main__":
    user = User(info={"name": "xing", "company": "mooc"})
    print(user.name)    # 没有这个name属性，执行内部的方法


 8.3 属性描述符和属性查找过程

  属性描述符：  只要实现了 __get__, _set__, __delete__ 这三个魔法函数，就是属性描述符

如果全部实现，则为数据属性描述符，如果只实现__get__，则为非数据属性描述符

import numbers
class IntField:
  # 数据描述符
    def __get__(self, instance, owner):
        return self.value

    def __set__(self, instance, value):
        if not isinstance(value, numbers.Integral):
            raise ValueError("int value need")
        if value<0:
            raise ValueError("positive value need")
        self.value = value

    def __delete__(self, instance):
        pass


class NonDataIntField:
    # 非数据属性描述符
    def __get__(self, instance, owner):
        return self.value

class User:
    age = IntField()
    # age = NonDataIntField()


if __name__ == "__main__":
    user = User()
    user.age = 30
    print(user.__dict__)  # {}
    user.__dict__['age'] = 20
    print(user.__dict__)  # {'age': 20}
    print(user.age)  # 30


属性查找过程：

如果user是某个类的实例，那么user.age（以及等价的getattr(user,'age'）)

首先调用__getattribute__。如果类定义了__getattr__方法，
那么在__getattribute__ 抛出AttributeError 的时候就会调用到__getattr__
而对于描述符（__get__）的调用，则是发生在__getattribute__内部的

（1）如果“age”是出现在User或者 其基类的__dict__中，且age是data descriptor, 那么调用其__get__方法，否则：
（2）如果"age" 出现在obj的__dict__中，那么直接返回obj.__dict__['age'],否则
（3）如果"age"出现在User或者其基类的__dict__中
（3.1）如果age是non_data descriptor， 那么调用其__get__方法，否则
（3.2）返回 __dict__['age']
（4） 如果User有__getattr__方法，调用 __getattr__方法，否则
（5）抛出AttributeErroe


8.4 __new__和__init__的区别

 __new__ 是用来控制对象的生成过程，在对象生成之前
 __init__ 方法是用来完善对象的
 如果__new__方法不返回对象，则不会调用__init__函数

class User:
    # __new__ 是在python2.2之后才有的这个方法
    def __new__(cls, *args, **kwargs):
        print("in new ")
        return super().__new__(cls)

    def __init__(self, name):
        print("in init")
        self.name = name


if __name__ == "__main__":
    user = User(name="xing")



---**********---------*********---------

 元类编程： 为了动态的创建类

  元类：就是创建类的类  对象<-class(对象) <-type


def say(self):
     pass

即： type("User",(BaseClass,),{"name":"xing"，"say":say}
这里如果值是方法，就不用加方法的括号

在实际开发中，不直接使用type创建元类，而是使用继承type的方式，从而实现元类
比如：

  class MetaClass(type);
  	pass
  class User(metaclass=MetaClass):
        pass
  这里继承type的类就叫元类， 必须要在类中使用metaclass


 元类编程的作用：

 通过元类去控制类的创建过程，这样代码的分离性比较好

  就是将需要提前检查的操作，放在元类里面，省去了自生定义检查的麻烦。比如，在实例化之前就检查是否继承了抽象基类，是否重写了抽象基类里的方法，如果没有就直接抛出异常，不用实例化了

 在Python中的类的实例化过程，会首先寻找metaclass,通过metaclass去创建user类

  在寻找metaclass过程中，首先在当前类中寻找，如果没有就去继承的基类中找，然后最终找到metaclass

如果最终都没有找到metaclass，那么类就会通过type去创建类对象，创建实例


class MetaClass(type):
    def __new__(cls, *args, **kwargs):
        return super().__new__(cls,*args, **kwargs) # 可以在实例化之前做一些操作

# 元类编程就是将检查、或者实例化之前的操作委托给元类来做，而不用自己实现这个__new__方法

class User(metaclass=MetaClass):
    def __init__(self, name):
        self.name = name

    def __str__(self):
        return "user"


if __name__ == "__main__":
    user = User("xing")
    print(user)



--------------------*******************************---------------------

			第九章  迭代器和生成器

9.1 python中的迭代协议

  什么是迭代协议；迭代器和可迭代类型，是不一样的

 迭代器是什么 ？迭代器是访问集合内元素的一种方式， 一般用来遍历数据。
迭代器和以下标的访问方式不一样，迭代器是不能放回的，迭代器提供了一种惰性方式数据的方式，即：只有在访问数据的时候才去计算或者获取数据

  [] list 等可迭代的类型都是实现了迭代协议的，所以才能使用for循环，即这个__iter__方法，使用 __getitem__ 方法可以实现迭代，还能进行切片操作

  实现了Iterable类中的 __iter__方法，就是可迭代的
  实现了__next__才是迭代器 Iterator

 Tterator 迭代器继承了Iterable的基础上，实现了抽象方法 @abstractmethod __next__方法，就是返回迭代中的下一个元素,并且重载了 __iter__方法
 

from collections.abc import Iterable, Iterator

 a=[1,2]
 print(isinstance(a, Iterable))  # True
 print(isinstance(a, Iterator))  # False

9.2 迭代器和可迭代对象

  for 循环实现迭代的过程，首先解释器查找 __iter__方法，如果没有内部自己建立一个iter对象，然后退化到查找 __getitem__方法，如果还是没有就抛出异常

for 内部处理：

company = Company（）:
for item in compan:
     print(item)

内部实现：
  my_itor =iter(company) # 转为可迭代对象
  while(True):
      try:
           print(next(my_itor))
      except StopItreation:
           pass
  

**************************
自己实现一个迭代器
**************************
from collections.abc import Iterator


class Company(object):
    def __init__(self, employee_list):
        self.employee = employee_list

    def __iter__(self):
        return MyIterator(self.employee)


class MyIterator(Iterator):
    """自己实现迭代器"""
    def __init__(self, employee_lsit):
        self.iter_list = employee_lsit
        self.index = 0

    def __next__(self):
        # 真正返回迭代值的逻辑
        try:
            word =self.iter_list[self.index]
        except IndexError:
            raise StopIteration
        self.index +=1
        return word


if __name__ == "__main__":
    company = Company(["xing1", "xing2", "xing3"])
    for item in company:
        print(item)



9.3 生成器函数的使用
 
  生成器函数，就是函数里只要有 yield 关键字，就是生成器函数

  yield 返回一个生成器对象，与函数理解一样，但是不同的是可以一直返回下去，而函数只能返回一次，只要在循环里面，可以一直执行下去，而不是跳出函数。
   yield 生成器对象是在编译字节码的时候就已经产生了，生成器对象是实现了迭代器协议的

# 因此使用 yield ，只有在计算的时候才会产生值，并不会占用内存
# 同时可以一步一步的得到每一步的值

def gen_func():
    yield 1
    yield 2
    yield 3
 # 惰性求值，延迟求值提供了可能


def func():
    return 1


# 只能返回最后的值，不能看见其过程
def fib(index):
    # 斐波拉契 1 1 2 3 5 8
    if index <= 2:
        return 1
    else:
        return fib(index-1) + fib(index-2)


# 这个可以看见实现的过程
def fib2(index):
    re_list = []
    n, a, b = 0, 0, 1
    while n < index:
        re_list.append(b)
        a, b = b, a+b
        n += 1
    return re_list


# 但是当index 非常大的时候，就会占用极大的内存
# 因此使用 yield ，只有在计算的时候才会产生值，并不会占用内存
# 同时可以一步一步的得到每一步的值
def gen_fib(index):
    n, a, b = 0, 0, 1
    while n < index:
        yield b
        a, b = b, a + b
        n += 1


if __name__ == "__main__":
    gen = gen_func()
    for value in gen:
        print(value)
    print(func())

    print(fib(10))
    print(fib2(10))
    for index in gen_fib(10):
        print(index)

*******************************
     9.4 生成器的原理
***************************


# python 中函数的工作原理
import inspect,dis
frame = None
def foo():
    bar()
def bar():
    global frame
    frame = inspect.currentframe()

print(dis.dis(foo))  # foo 的字节码对象，实现的过程
foo()
print(frame.f_code.co_name)   # bar
print(frame.f_back.f_code.co_name)    # foo

# Python.exe 会用一个叫做PyEval_EvalFrameEx(c函数）去执行foo函数，首先会创建一个栈帧（stack frame）
# 这个栈帧是一个上下文， 栈帧对象，然后变成函数变成 字节码对象
# 然后在栈帧对象的上下文里面运行这个foo函数的字节码
# 当foo调用子函数 bar ，又会创建一个栈帧，然后交给这个新的栈帧对象，在新的栈帧对象里运行这个bar函数的字节码
# 所有的栈帧都是放在堆内存上，即不会随着函数的消失而释放内存，这就决定了栈帧可以独立于调用者存在，意思就是，当foo不存在了，也能在bar栈帧中有指针指向他，从而控制他
# 因此，对于函数的控制过程非常的精确


生成器的原理：

print("--------------------")

def gen_func():
    yield 1
    name = "xing"
    yield 2
    age =25
    return "company"
gen = gen_func()
print(gen.gi_frame.f_lasti)
print(gen.gi_frame.f_locals)
next(gen)
print(gen.gi_frame.f_lasti)
print(gen.gi_frame.f_locals)
next(gen)
print(gen.gi_frame.f_lasti)
print(gen.gi_frame.f_locals)
print("--------------------")
print(dis.dis(gen))  # gen 的字节码对象，实现的过程


# 静态语言中，函数放在栈里面的，当函数调用完成后，会全部销毁，而Python是放在堆里面的，并不会销毁
# yield 的调用，都会内部暂停，有了变量保存的位置，以及最近的代码执行的位置，就能控制函数的暂停，独立于调用者，只要拿到栈帧对象就能控制函数往前走
# yield 每次调用都会暂停，然后在堆里面记录运行前后函数的位置lasti和locals，可以控制整个函数的暂停
# 在任何模块里面，只要拿到生成器对象都可以去恢复它，暂停他，在任何对方都可以控制他， 因此有了协程的概念



9.5  生成器在UserList中的应用


  list使用C语言写的，看不见，因此可以使用UserList,这个是使用Python实现了一遍list，因此最好不要继承重写List，因为其内部做了很多优化，有些关键函数是不能够覆盖的。

UserList中的__iter__方法：
from collections import UserList
def __iter__(self):
    i =0
    try:
        while True:
	    v = self[i]
	    yield v
	    i += 1
    except IndexError:
	return


9.6 生成器如何读取大文件

 # 需求： 读取500G的文件，但是只有一行，分隔符为 {|}

def myreadlines(f, newline):
    buf = ""
    while True:
        while newline in buf:
            # 这个循环是为了在读取的4096个字符中，查找是否还存在着 分隔符{|}
            pos = buf.index(newline)
            yield buf[:pos]
            buf = buf[pos+len(newline):]
        chunk = f.read(4096)  # read方法读取4096个字符，下一次读取的时候根据偏移值，读取下一个4096

        if not chunk:
            # 这里表示读到了末尾了
            yield buf
            break
        buf += chunk


with open("xing.txt", "r", encoding="utf-8") as f:
    for line in myreadlines(f, "{|}"):
        print(line)



----------------------***************************-----------------

  			第十章  Python socket编程


10.1 弄懂HTTP、Socket、TCP 这几个概念

  socket 本身是不属于计算机网络的协议，是连接应用层和网络层，使得自己的应用可以直接脱离应用层，直接和传输层 的TCP UDP和以下的协议打交道，定义自己的协议。
  http 是一个单向的过程，假如A向B发起一个请求，然后B发送给A响应，只有这一个过程，如果B再想向A发送消息，是不行的，B只能响应A的请求
   
  因此 使用Socket，直接使用底层的协议，TCP，只要连接好就能一直保持数据的双向传输

-----------------------------------------------------------------
   OSI层     |     功能                  |   TCP/IP协议
------------------------------------------------------------------
   应用层    | 文件传输、电子邮件、文件服务|  HTTP,FTP,SMTP,DNS,TeInet
--------------------------------------------------------------------
   传输层    | 提供端对端的接口           |  TCP,UDP
-------------------------------------------------------------------
   网络层    | 为数据包选择路由           |  IP, ICMP 等
--------------------------------------------------------------------
  数据链路层 | 传输有地址的帧，错误检测功能|  ARP 等
--------------------------------------------------------------------
  物理层     | 物理媒体                   |  1000BASE-SX 等 
--------------------------------------------------------------------


10.2  socket的 client和server实现通信

  服务端：                 客户端：

  Socket
  bind(协议，地址，端口)  #元组类型
  listen()
  
  accept()                  connect(地址，端口) # 元组类型
  recv()                    send()
  send(byte类型)             recv()
  close()                    close()

10.3  socket实现聊天和多用户连接
    

        服务器：
import socket, threading

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(("0.0.0.0", 8000))
server.listen()
flag = False


def handle_sock(sock, addr):
    # 获取从客户端发送的数据
    # 一次获取1k的数据
    data = sock.recv(1024)
    print(data.decode("utf8"))
    # 接收并发送给客户端
    re_data = input()
    if re_data == "exit":
        global flag
        flag = True

    sock.send(re_data.encode("utf8"))


while True:
    sock, addr = server.accept()
    client_thread = threading.Thread(target=handle_sock, args=(sock, addr))
    client_thread.start()
    if flag:
        break

server.close()
sock.close()

      客户端：

import socket


client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
client.connect(("127.0.0.1", 8000))

while True:
    re_data = input()
    if re_data == "exit":
        break
    client.send(re_data.encode("utf8"))
    data = client.recv(1024)
    print(data.decode("utf8"))

client.close()


10.4 socket 模拟http请求

  from urllib.parse import urlparse
  import socket


def get_url(url):
    # 通过socket请求html
    url = urlparse(url)
    host = url.netloc
    path = url.path
    if path =="":
        path = "/"

    # 建立socket连接
    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client.connect((host,80))

    client.send("GET {} HTTP/1.1\r\nHost:{}\r\nConnection:close\r\n\r\n".format(path, host).encode("utf8"))

    data = b""
    while True:
        d = client.recv(1024)
        if d:
            data += d
        else:
            break
    data = data.decode("utf8")
    # 为了去掉 head部分
    html_data = data.split("\r\n\r\n")[1]
    print(html_data)
    client.close()


if __name__ == "__main__":
    url = "http://www.baidu.com"
    get_url(url)

 还是一个道理，http只能连接一次后就断开了，如果需要继续连接传输信息，就需要重新发送请求，就像request一样。
  而socket 就是为了可以一直连着不断开




------------------------*********************-------------------

			第十一章  多线程

11.1 Python中的GIL

  GIL(global interpreter lock)全局解释器锁  基于cpython的
  Python中一个线程对应与c语言中的一个线程
  GIL 使得同一个时刻只有一个线程在一个cpu上执行字节码，无法将多个线程映射到多个cpu上执行
   因此无论运行多少个线程，都只能运行在一个cpu上面，因此都说Python很慢

  但是 GIL也会根据执行的字节码行数（比如1000行）以及时间片（比如15ms）释放GIL，GIL在遇到IO的操作时，也会主动释放锁
 
 

11.2 多线程编程

 对于IO编程来说，多线程和多进程性能差别不大，只是多线程比较轻量级

"""需求1 :"""
 当主线程退出的时候，子线程就 Kill 掉，意思就是当主线程执行完毕之后，不管子线程是否执行完，直接关闭程序
# 使用 setDaemon（True）方法
   thread2.setDaemon(True)
# 如果不设置的话，就是主线程执行完之后，子线程继续运行，即子线程和主线程是同步的，与下面需求2 不一样

"""需求2："""
# 希望当子线程执行完之后，才去继续执行接下来的主线程
# 使用 join()方法
# 他会在此处进行阻塞，直到该线程（join）执行完
thread1.join()

 有两种方法：

一、 通过Thread类实例化

import threading
import time


def get_detail_html(url):
    print("get detail html start")
    time.sleep(2)
    print("get detail html end")


def get_detail_des(url):
    print("get detail des start")
    time.sleep(3)
    print("get detail des end")


if __name__ == "__main__":

    thread1 = threading.Thread(target=get_detail_html, args=("",))
    thread2 = threading.Thread(target=get_detail_des, args=("",))
    start_time = time.time()
    # % 守护线程 %
    thread2.setDaemon(True)
    # 线程启动
    thread1.start()
    thread2.start()

    thread1.join()
    thread2.join()

    end_time = time.time()
    print("main process end {}".format(end_time - start_time))



二、 通过继承Thread来实现多线程

"""重载run方法"""


class Get_detail_html(threading.Thread):
    def __init__(self, name):
        super().__init__(name=name)

    def run(self):
        print("get detail html start")
        time.sleep(2)
        print("get detail html end")


class Get_detail_des(threading.Thread):
    def __init__(self, name):
        super().__init__(name= name)

    def run(self):
        print("get detail des start")
        time.sleep(4)
        print("get detail des end")


if __name__ == "__main__":
    thread3 = Get_detail_html("get_detail_html")
    thread4 = Get_detail_des("get_detail_des")

    start_time = time.time()
    # % 守护线程 %
    thread4.setDaemon(True)
    # 线程启动
    thread3.start()
    thread4.start()

    thread3.join()
    thread4.join()

    end_time = time.time()
    print("main process end {}".format(end_time - start_time))



11.3 线程通信 
 
  1、共享变量
   使用全局变量的方式，实现线程间的通信，但是当多个线程使用同一个变量的时候，就容易出现很多的问题，因此，并不推荐使用变量的通信，但是有些时候，变量是dic等其他类型的时候，也可以使用， 最好使用queue
   共享变量的通信，对于线程来说并不安全，尤其是List类型等。

import threading
import time


detail_url_list = []


def get_detail_html(detail_url_list):
    while True:
        if len(detail_url_list):
            url = detail_url_list.pop()
            print("get detail html start")
            time.sleep(2)
            print("get detail html end")


def get_detail_url(detail_url_list):
    while True:
        for i in range(10):
            print("get detail url start")
            time.sleep(3)
            detail_url_list.append("http://baidu.com/{}".format(i))
            print("get detail url end")


if __name__ == "__main__":

    thread1 = threading.Thread(target=get_detail_html,args=(detail_url_list,))
    start_time = time.time()
    thread1.start()
    for i in range(10):
        thread2 = threading.Thread(target=get_detail_url, args=(detail_url_list,))
        thread2.start()

    end_time = time.time()
    print("main process end {}".format(end_time - start_time))


  2. 使用Queue 进行通信

  queue的方式，进行线程间的同步， 对于线程来说是安全的，因为queue的get和put 方法，实质上都是实现了 deque的方式，对于双端队列来说，从字节码的级别上都是安全的

   queue的方法：
 get（） 方法，取数据，是一个阻塞方法，就是当取得为空的时候，就会阻塞在这，直到取到数据
 put() 方法， 放数据
qsize() ，返回队列的长度
empty（），判断队列是否为空
full()， 判断队列是否满了，如果满了，put就会阻塞在这里，直到队列可以存放数据
put_nowait()   就是存放数据的时候，不用等到他完成之后才返回，这是一个异步方法
join() 方法，就是将这个阻塞在这里，直到完成之后，才会执行继续下去的主线程
task_done()  如果想要 结束join（） 方法，必须使用 task_done()方法，不然，join()方法是不会退出的。
  
因此 task_done()和join()方法是成对出现的

import threading
import time
from queue import Queue


def get_detail_html(queue):
    while True:
        url = queue.get()  # get 方法，是一个阻塞方法，就是当取得为空的时候，就会阻塞在这，直到取到数据
        print("get detail html start")
        time.sleep(2)
        print("get detail html end")


def get_detail_url(queue):
    while True:
        for i in range(10):
            print("get detail url start")
            time.sleep(3)
            queue.put("http://baidu.com/{}".format(i))
            print("get detail url end")


if __name__ == "__main__":
    detail_url_queue = Queue(maxsize=100)

    thread1 = threading.Thread(target=get_detail_html, args=(detail_url_queue,))
    start_time = time.time()
    thread1.start()
    for i in range(10):
        thread2 = threading.Thread(target=get_detail_url, args=(detail_url_queue,))
        thread2.start()

    end_time = time.time()
    print("main process end {}".format(end_time - start_time))


11.4  线程同步


  为了使得线程安全，并且使得线程同步，需要加锁，但是加了锁之后就会影响 性能，获取锁和释放锁需要一定的时间。 并且容易造成死锁

 1..加锁  Lock()
  lock = Lock()
  lock.acquire() # 获取锁
  lock.release() # 释放锁
 
  当连续两次获取锁的时候，就容易造成死锁，即：
   lock.acquire()
   lock.acquire()

 或者：

   A(a, b)    B(a,b)  这两个都需要获取a,b的资源，然后一人拿了一个锁，就会造成死锁。
   A.acquire(a)           B.acquire(b)
   A.acquire(b)           B.acquire(a)
   
   
2... 加锁  Rlock()

   这是一个可重入的锁，在同一个线程里面，可以连续调用多次acquire，一定要注意acquire的次数要和release的次数相等。
  为了避免死锁，故 使用 Rlock()， 这个锁可以连续多次获取锁，但是释放锁的次数必须和获取锁的次数一致 ，即：
   lock.acquire()
   lock.acquire()
   lock.release()
   lock.release()

3...Condition 

 condition 内部有 __enter__和__exit__方法，因此可以使用with语句

  condition的启动顺序很重要，
  在调用with condition 或者 self.condition.acquire() 之后，才能调用wait 或者 notify 方法 ， 因为在wait 方法之前，必须要有一把锁
   condition有两层锁，一层底层锁是在condition初始化的时候就加上了一把锁，这把锁在线层调用wait方法的时候才会释放，这是为了多个线程同一时间只允许一个线程运行，然后第二把锁就是wait的时候 加上了一把锁，需要用 notify 来进行唤醒。


import threading
from threading import Condition


class XiaoAi(threading.Thread):
    def __init__(self, condition):
        super().__init__(name="小爱")
        self.condition = condition

    def run(self):
        with self.condition:   # 或者 self.condition.acquire()
            self.condition.wait()
            print("{}: 在".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 好啊".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 一岁一枯荣".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 春风吹又生".format(self.name))


class TianMao(threading.Thread):
    def __init__(self, condition):
        super().__init__(name="天猫精灵")
        self.condition = condition

    def run(self):
        with self.condition:  # 或者 self.condition.acquire()

            print("{}: 小爱同学".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 我们来对诗吧".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 离离原上草".format(self.name))
            self.condition.notify()
            self.condition.wait()

            print("{}: 野火烧不尽".format(self.name))
            self.condition.notify()


if __name__ == "__main__":
    condition = threading.Condition()
    xiaoai = XiaoAi(condition)
    tianmao = TianMao(condition)

    xiaoai.start()
    tianmao.start()



4...Semaphore

  Semaphore 是用于控制进入数量的锁
 文件，读，写，写一般只用于一个线程，读可以允许有多个线程。

semap = threading.Semaphore(3)  # 只允许一次性 三个线程

self.semap.acquire()  # 获取三个线程的锁
self.semap.release()  # 释放锁

import threading
import time


class HtmlSplider(threading.Thread):
    def __init__(self, url, semap):
        super().__init__()
        self.url = url
        self.semap = semap

    def run(self):
        time.sleep(2)
        print("got html success")
        self.semap.release()


class UrlProducer(threading.Thread):
    def __init__(self, semap):
        super().__init__()
        self.semap = semap

    def run(self):
        for i in range(20):
            self.semap.acquire()
            thread = HtmlSplider("http://baidu.com/{}".format(i), self.semap)
            thread.start()


if __name__ == "__main__":
    semap = threading.Semaphore(3)  # 只允许一次性 三个线程
    url = UrlProducer(semap)
    url.start()


11.5  ThreadPoolExcutor线程池

  线程池：
   就是主线程可以获取某一个线程的状态或者某一个任务的状态，以及返回值
   当一个线程完成的时候我们主线称能够立即知道
   futures可以让多线程和多进程的编码接口一致

 submit（） 通过submit函数提交到线程池中，submit是立即返回
该函数是一个非阻塞的方法，就是只提交到线程池中，不管后续的结果

 cancel() 函数是一个阻塞的方法，线程在运行或者结束的时候是不能进行取消的，只有在线程还没有开始的时候才能取消。

 done 方法由于判定某个任务是否完成
 result 方法可以获取线程的执行结果，是一种阻塞方法
 wait () 可以使主线程进行阻塞，意思就是可以指定线程完成的情况然后继续执行接下来的主线程。wait的return_when参数可以是：FIRST_COMPLETED 完成一个后执行主线程、ALL_COMPLETED 完成所有的线程执行完在执行、或者其他的

 如果想要获取已经完成的线程的返回值：
   1. 使用as_completed() 
     返回一个future对象，需要用future.result得到返回的结果
     返回的结果顺序是 ： 哪一个先完成就返回哪一个
   2. 使用executor.map()
      直接返回的是结果，不需要用result函数
      返回的结果顺序是： 根据传入的map里的参数的顺序


from concurrent.futures import ThreadPoolExecutor, as_completed, wait
import time


def get_html(times):
    time.sleep(times)
    print("get {} page success".format(times))
    return times

#         第一部分
# executor = ThreadPoolExecutor(max_workers=2)
# task1 = executor.submit(get_html, 2)
# task2 = executor.submit(get_html, 3)
#
# print(task1.done())
# time.sleep(2)
# print(task1.done())
#
# print(task2.result())

#           第二部分
"""
如果想要获取已经完成的线程的返回值：
   1. 使用as_completed() 
     返回一个future对象，需要用future.result得到返回的结果
     返回的结果顺序是 ： 哪一个先完成就返回哪一个
   2. 使用executor.map()
      直接返回的是结果，不需要用result函数
      返回的结果顺序是： 根据传入的map里的参数的顺序
"""
# 1. 使用as_completed()
executor = ThreadPoolExecutor(max_workers=2)
urls = [3, 2, 4]
all_task = [executor.submit(get_html, (url)) for url in urls]

# wait 函数，对主线程进行阻塞
wait(all_task, return_when=FIRST_COMPLETED)
print("main")

for future in as_completed(all_task):
    data = future.result()
    print("get {} page complete - as_completed".format(data))

# 2.使用executor.map()
for data in executor.map(get_html, urls):
    print("get {} page complete - map".format(data))


11.6 ThreadPoolExcutor 源码分析

from concurrent.futures import Future
 主要是 Future 和 _WorkItem

future :未来对象，在当前可能没有完成，但在将来的某个时候会完成，task的返回容器


11.7 多进程和多线程对比


  耗费CPU的操作，如：计算机计算、图形处理等，使用多进程编程
  对于IO操作来说，使用多线程编程
  进程切换代价要高于线程切换

  ProcessPoolExecutor 多进程进程池编程 与ThreadPoolExecutor多线程编程 接口都是一样的，

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import time

# 1 对于耗费cpu的操作，多进程优于多线程
def fib(n):
    if n <= 2:
        return 1
    return fib(n-1) + fib(n-2)


# if __name__ == "__main__":
#     with ProcessPoolExecutor(3) as executor:
#         all_task = [executor.submit(fib, (num)) for num in range(30, 35)]
#         start_time = time.time()
#         for future in as_completed(all_task):
#             data = future.result()
#             print("exe result {}".format(data))
#
#         end_time = time.time()
#         print("last_time : {}".format(end_time-start_time))


# 2 对于io操作来说，多线程优于多进程

def random_sleep(n):
    time.sleep(n)
    return n


if __name__ == "__main__":
    with ThreadPoolExecutor(3) as executor:
        all_task = [executor.submit(random_sleep, (n)) for n in [2]*10]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print("exe result {}".format(data))

        end_time = time.time()
        print("last_time : {}".format(end_time-start_time))



11.8  multiprocessing 编程

 其实使用 ProcessPoolExecutor 比较多，因为较为方便，用法比较灵活，但是其内部底层都是使用的 multiprocessing 进行的编程

import multiprocessing
import time
# 多进程编程


def get_html(n):
    time.sleep(n)
    print("sub_progress success")
    return n


if __name__ == "__main__":
    """ 一般与多线程一样的使用"""
    # progress = multiprocessing.Process(target=get_html, args=(2,))
    # progress.start()
    # print(progress.pid)  # 显示进程的Pid
    # progress.join()
    # print("main progress end")

    """使用进程池"""
    pool = multiprocessing.Pool(multiprocessing.cpu_count())
    # result = pool.apply_async(get_html, args=(2,))  # 就是进程的提交
    #
    # pool.close()  # 使用完之后，要将进程池关闭，否则会出错
    # # 等待所有的进程任务完成
    # pool.join()
    # print(result.get())


     # imap
    # 打印的结果和 添加的顺序是一样的，和线程池里面的 map方法是一样的。
    # for result in pool.imap(get_html,  [1, 3, 2]):
    #     print("{} sleep success".format(result))
    #
    # imap_unordered()
    # 打印的结果: 谁先完成，就打印谁，和线程池里面的 as_completed 是一样的。
    for result in pool.imap_unordered(get_html,  [1, 3, 2]):
        print("{} sleep success".format(result))



11.9 进程间的通信 (Queue, Pipe,Manager)

1 ..queue:

 多进程间的数据是互相隔离的，当fork 的时候，会将进程里的所有全局变量都复制一份到子进程中，当改变一方的时候，并不会影响另一方，进程间是相互独立的

 因此，全局变量不能使用于多进程通信，可以适用于多线程通信

 Queue 
   进程间的通信 不能像线程使用queue下面的Queue，而是使用 multiprocessing 下面的Queue
 queue = multiprocessing.Queue()

 而且 multiprocess 下的Queue 不能用于进程池的通信即（multiprocessing.Pool）,进程池的通信只能使用 multiprocessing下的Manager下的Queue
  queue = multiprocessing.Manager().Queue()


************---------------------------------*************
 因此有3个Queue
  from queue import Queue
  from multiprocessing import Queue
  from multiprocessing import Manager
--> Manager().Queue()
*************--------------------------------**************


import multiprocessing, time
from multiprocessing import Process, Queue, Manager
# queue = multiprocessing.Manager().Queue()


def producer(queue):
    queue.put("a")
    time.sleep(2)


def consumer(queue):
    time.sleep(2)
    data = queue.get()
    print(data)


if __name__ == "__main__":
    queue = multiprocessing.Queue(10)
    my_producer = multiprocessing.Process(target=producer, args=(queue,))
    my_consumer = multiprocessing.Process(target=consumer, args=(queue,))

    my_producer.start()
    my_consumer.start()

    my_producer.join()
    my_consumer.join()


2..pipe
 
  通过pipe实现进程间的通信，pipe只能适用于两个进程
  pipe 的性能要高于queue

reveive_pipe, send_pipe = multiprocessing.Pipe()

pip.send("a")
pip.recv()

import multiprocessing
def producer(pip):
    pip.send("a")


def consumer(pip):
    print(pip.recv())


if __name__ == "__main__":
    reveive_pipe, send_pipe = multiprocessing.Pipe()
    my_producer = multiprocessing.Process(target=producer, args=(send_pipe,))
    my_consumer = multiprocessing.Process(target=consumer, args=(reveive_pipe,))

    my_producer.start()
    my_consumer.start()

    my_producer.join()
    my_consumer.join()



3..进程的共享内存

进程间的共享内存， 在multiprocessing下的Manager（）里面
process_dict = multiprocessing.Manager().dict()
我们在进行操作的时候，要注意考虑到同步，尤其是里面的Lock、Rlock等

例子：
def add_data(p_dict, key, value):
    p_dict[key] = value


if __name__ == "__main__":
    process_dict = Manager().dict()
    first_process = multiprocessing.Process(target=add_data, args=(process_dict, "xing1", 23))
    second_process = multiprocessing.Process(target=add_data, args=(process_dict, "xing2", 20))
    first_process.start()
    second_process.start()
    first_process.join()
    second_process.join()
    print(process_dict)







--------------------------***************-----------------------

			第 十二 章 协程和异步io

12.1 并发、并行、同步、异步、阻塞、非阻塞

  并发： 是指在同一个时间段内，有几个程序在同一个cpu上运行，但是任意时刻只有一个程序在cpu上运行（就是同一时刻，cpu只能运行一个程序）
  并行： 是指在任意时刻点上，有多个程序同时运行在多个cpu上（多cpu，多程序）

  同步：是指代码调用IO操作时，必须等待IO操作完成才返回的调用方式
  异步：是指代码调用IO操作时，不必等待IO操作完成后就返回的调用方式
 
  阻塞：是指调用函数时候 当前线程被挂起
  非阻塞：是指调用函数时候 当前线程不会被挂起，而是直接返回。


12.2 I/O多路复用（select，poll，epoll）


 C10K 问题： 如何在一颗1GHZ CPU，2G内存，1gbps网络环境下，让单台服务器同时为1万个客户端提供FTP服务

 Unix 下五中I/O模型：

  阻塞式I/O
  非阻塞式 I/O
  I/O复用
  信号驱动式I/O
  异步I/O（POSIX的aio_系列函数）


1、阻塞式I/O

  极大地浪费cpu资源， 
  但并不会消耗cpu（cpu没有运行）

2、非阻塞式 I/O

   cpu没有继续等待，比如连接的时候（connect 三次握手）但是在不停的询问连接是否建立好，需要while循环不停的去检查状态，耗费cpu

   但是当做计算任务或者再次发起其他的连接请求的时候就很有用了（即不需要检查当前的连接，可以去做其他的事情，节省时间，提高效率）

3、I/O复用

   select 也是一个阻塞式的方法，但是和while for循环有很大的区别，可以同时监听多个状态，一旦有一个状态发生变化，立马可以去处理
   当数据准备好之后 返回一个命令，然后进行数据复制处理
epoll 只有在Linux中支持

4.信号驱动式I/O

  基于信号来的，操作系统主动给处理程序发送信号，用的很少

5. 异步I/O（POSIX的aio_系列函数）

   这是真正的异步IO，当数据复制完后才发送命令，但是相对于IO复用来说，提升并不是很明显，因此现在的异步IO基本上大多数都是 IO复用


   IO多路复用：
  
   select，poll，epoll 都是IO多路复用的机制。 IO多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select， poll， epoll本质上都是同步IO，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步IO则不需自己负责进行读写，异步IO的实现会负责把数据从内核拷贝到用户空间。

  select：

  select是用的最为广泛的，支持所有的操作系统。 select函数监视的文件描述符分为3类，分别是writefds, readfds, 和 exceptfds 。抵用select函数会阻塞，直到有描述符就绪（有数据可读、可写、或者有except），或者超时（timeout 指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。
   select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个有点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Liunx上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样会造成效率的降低。

   poll：
  
   不同于select使用三个位图来表示三个fdset的方式，poll使用一个pollfd的指针实现。
  pollfd结构包含了要监视的event和发生的event，不在使用select“参数-值”传递的方式。同时，pollfd没有最大数量限制（但是数量过大后性能也是会下降）。和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符
   从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

  epoll:
  
  epoll 是在Linux2.6内核中提出的，只支持Linux系统，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的时间存放到内核的一个时间表中，这样在用户空间和内核空间的copy 只需一次


 epoll 并不代表一定比select好：

  在并发高的情况下，连接活跃度不是很高，epoll比select好（比如 web系统）
  在并发性不高，同时连接很活跃，select比epoll好 （比如 游戏）



12.3 select+回调+时间循环获取

 针对单线程，高并发，当线程多的时候，线程切换很慢，运行速度也会变慢，但采用这种 select + 回调+事件循环 的方式，无论有多少个，运行都非常的快。


"""
通过 select 实现http请求 （非阻塞）
   select + 回调 + 事件循环
   单线程
   并发性很高
   事件循环是心脏，不会阻塞建立连接或者等待网络请求，一旦完成直接调用回调函数，
    完全是cpu在操作，cpu操作远远高于网络 IO ， 效率不是一般的高
    单线程 省去了线程切换的开销，减小了内存占用， 线程的内存远高于 回调函数的模式
"""
from urllib.parse import urlparse
import socket
from selectors import DefaultSelector, EVENT_WRITE, EVENT_READ

selector = DefaultSelector()
urls = ["http://www.baidu.com"]
flag_stop = False


class Fetcher:
    def connected(self, key):
        """回调函数"""
        selector.unregister(key.fd)  # key.fd 是self.client.fileno()注册后的返回值
        self.client.send("GET {} HTTP/1.1\r\nHost:{}\r\nConnection:close\r\n\r\n".format(self.path, self.host).encode("utf8"))
        selector.register(self.client.fileno(), EVENT_READ, self.readable)

    def readable(self, key):
        """回调函数"""
        # 接收数据
        d = self.client.recv(1024)
        if d:
            self.data += d
        else:
            selector.unregister(key.fd)
            data = self.data.decode("utf8")
            html_data = data.split("\r\n\r\n")[1]  # 为了去掉 head部分
            print(html_data)
            self.client.close()
            # 返回数据之后，删除此条url
            urls.remove(self.url_splider)
            # 判断urls是否为空
            if not urls:
                global flag_stop
                flag_stop = True

    def get_url(self, url):
        self.url_splider = url
        url = urlparse(url)
        self.host = url.netloc
        self.path = url.path
        self.data = b""
        if self.path == "":
            self.path = "/"

        # 建立socket连接
        self.client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.client.setblocking(False)  # 设置为非阻塞的
        try:
            self.client.connect((self.host, 80))
        except BlockingIOError as e :
            pass

        # 注册 （socket文件描述符， 事件（读或写）， 回调函数）
        selector.register(self.client.fileno(), EVENT_WRITE, self.connected)


def loop():
    """
    事件循环，不停的请求socket的状态，并调用对应的回调函数
    """
    # 1、select 本身是不支持register模式的
    # 2、socket 状态变化以后的回调是由程序员完成的
    while not flag_stop:
        ready = selector.select()  # 返回一个元素是元组的列表
        for key, mask in ready:  # 对列表进行迭代，实现元组的拆包
            # key 里面的就是这些
            # key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)
            call_back = key.data  # 就是回调函数。回调函数放在data里面的
            call_back(key)   # 执行回到函数


if __name__ == "__main__":
    fetcher = Fetcher()
    for url in urls:
        fetcher.get_url(url)
    loop()



12.5   回调之痛

  如果回调函数执行不正常该如何？
  如果回调里面还要嵌套回调怎么办？ 要嵌套很多层怎么办？
  如果嵌套了多层，其中某个环节出了错会造成什么后果？
  如果有个数据需要被每个回调都处理怎么办？
  怎么使用当前函数中的局部变量？
 .......

 总结起来：

  回调的缺点：1、可读性差，难以维护
              2、 共享状态管理困难
              3、 异常处理困难


12.6  协程

C10M问题：
   如何利用8核CPU，64G内存，在10gbps的网络上保持1000万并非连接


现在遇到的问题：
   1、回调模式编码复杂度高
   2、同步编程的并发性不高
   3、多线程编程需要线程间同步，lock
  

解决： 
   1、 采用同步的方式去编写异步的代码
   2、使用单线程去切换任务
	1.线程是由操作系统切换的，单线程切换意味着我们需要程序员自己去调度任务
	2、不在需要锁，并发行高， 如果单线程内切换函数，性能远高于线程切换，并发性更高


引入了协程：
  传统函数调用 过程A->B->C  ，当在函数A中去调用B的时候，A就会停止了，然后执行B，再去执行C，返回执行B，返回执行A，但如果其中再想执行C，就会在C中重新开始执行，一直到底。因为函数是放在栈里面的

  我们需要一个可以暂停的函数，并且可以在适当的时候恢复该函数的继续执行

出现了协程；



12.7 生成器进阶 send，close, throw方法
 
 send():

  生成器不止可以产出值，还可以接收值

  html = yield "http://baidu.com"
 
  html 即为接收的值，外部调用函数可以向他传递参数， 而“http://baidu.com”即为产出值

 send()方法： 可以传递值进生成器内部，同时还可以重启生成器执行到下一个yield，即实现next（）方法的功能。

但在调用send()发送非None值之前，我们必须启动一次生成器，方式有两种：1, gen.send(None) 2, next(gen)

def gen():
    html = yield "www.baidu.com**1"
    print(html)
    yield 2
    print("xing**3")

    return 4


if __name__ == "__main__":
    gen = gen()
    print(gen.send(None))
    # 等价于 print(next(gen))
    print(gen.send("xingxing**2"))

    print(next(gen))

 
close()方法：
  gen.close() 关闭生成器，但是如果后面还有yield,如果不处理，将不会出错，但是擅自使用try  except捕获GeneratorExit异常，会在 gen.close()上面抛出异常， 除非 raise StopIteration 异常； 同时GeneratorExit异常是继承自BaseException, 而不是Exception

因此不要擅自 try  except 异常


throw（） 方法：
   调用者向生成器中扔一个异常， 这里捕获的异常是向上抛的，这里的异常与close()中的不一样，需要我们去处理。

def gen():
    try:
        yield 1
    except Exception:
        pass
    yield 2
    yield 3
    return 5


if __name__ == "__main__":
    gen = gen()
    print(next(gen))
    gen.throw(Exception, "download error")
    print(next(gen))
    gen.throw(Exception, "download error")
    print(next(gen))
  


12.8 yield from

 在Python3.3 新加了yield from语法

yield from ：
     iter = yield from [iterable]

 1、直接将可迭代的iterable 一个一个的拿出来。

  def g1(iterable):
    yield iterable


def g2(iterable):
    yield from iterable


for value in g1(range(5)):
    print(value)

for value in g2(range(5)):
    print(value)


 2、 yield from 会在调用方与子生成器之间建立一个通道（这才是重点）

 main 调用方， g3(委托生成器） gen 子生成器

 def g3(gen):
    yield from gen

    
 def main():
    g = g3()
    g.send(None)


为什么要用yield from :?
  
  背后默默做了很多事，省去了很多的麻烦，其中最主要的就是 捕获了很多很多的异常，让我们不用去考虑yield完成后抛出的异常，比如 StopIteration, 



yield from 的关键：（帮我们做的事情）

  1、子生成器生产的值，都是直接传给调用方的： 调用方通过 .send() 发送的值都是直接传递给子生成器的，如果发送的None，则会调用子生成器的__next__()方法，如果不是None,则会调用子生成器的.send()方法。
  2、 子生成器退出的时候，最后的return ECPR，会触发一个StopIteration(EXPR) 异常；
  3、yield from表达式的值，是子生成器终止时，传递给StopIteration异常的第一个参数
  4、 如果调用的时候出现StopIteration异常，委托生成器会恢复运行，同时其他的异常会向上冒泡
  5、传入委托生成器的异常里，除了GeneratorExit之外，其他的所有异常全部传递给子生成器的.throw() 方法，如果.throw()的时候出现了StopIteration异常，那么恢复委托生成器的运行，其他的异常全部向上冒泡
  6、如果在委托生成器上调用 .close() 或传入GenerationExit异常，会调用生成器的 .close()方法， 没有的话，就不调用，如果在调用close()的时候抛出了异常，那么就向上冒泡，否则的话，委托生成器就抛出GeneratorExit异常。


12.10  async 和 await

 Python 3.5 之后为了将语义变得更加明确，就引入了async 和 await 关键词，用于定义原生的协程

定义了async 和 await 关键词之后，里面就不能在定义 yield了，同时只能用send 不能用next 来进行yield的预激

await 可以理解为 yield from

import types

@types.coroutine
def downloader(url):
    yield "xing2"


async def download(url):
    return "xing1"


async def download_url(url):
    html = await download(url)  # await 可以理解为 yield from
    html1 = await downloader(url)
    return html

if __name__ == "__main__":
    coroutine = download_url("http://www.baidu.com")
    coroutine.send(None)  # 这里只能用send 不能用next


12.11 生成器实现协程

生成器实现的协程中，不能使用time.sleep()等耗时的操作。
协程的调度依然是  事件循环 +协程模式， 协程是单线程

主要就是通过 yield from 将耗时的IO操作给yield出去，减少时间损耗

 查看生成器的状态
  import inspect
  inspect.getgeneratorstate(gen)
 
    GEN_CREATE:等待开始执行
    GEN_RUNNING:解释器正在执行，这个状态一般看不到
    GEN_SUSPENDED:在yield表达式处暂停
    GEN_CLOSED:执行结束




--------------------------*********************--------------------

			第十三章 asyncio 并发编程

13.1 事件循环

 asyncio 是Python用于解决异步io编程的一整套解决方案
  
 asyncio: 
 
  包含各种特定系统实现的模块化事件循环
  传输和协议抽象
  对TCP,UDP,SSL,子进程、延时调用以及其他的具体支持
  模仿futures模块 但适用于事件循环使用的Future类
  基于 yield from的协议和任务，可以让你用顺序的方式编写并发代码
  必须使用一个将产生阻塞IO的调用时，有借口可以把这个事件转移到线程池
  模仿threading模块中的同步原语、可以用在单线程内的协程之间



事件循环+回调（驱动生成器）+epoll(IO多路复用）

异步io编程其中典型的： tornado、gevent、twisted(scrap, djando channels)框架
tornado （可以直接实现web服务器），而 django + flask(uwsgi, gunicorn+nginx) 实现的
tornado可以直接部署， ：nginx+tornado


import asyncio


async def get_html(url):
    print("start get url ")
    await asyncio.sleep(2)  # 这里一定不能用time.sleep()方法，因为这是一个同步阻塞的方法。
    # 一定要用asyncio提供的sleep()方法。这样才不会造成阻塞
    print("end get url ")


if __name__ == "__main__":
    import time
    start_time = time.time()
    loop = asyncio.get_event_loop()  # 事件循环
    tasks = [get_html("http://baidu.com") for i in range(10)]
    loop.run_until_complete(asyncio.wait(tasks))  # wait接的是一个可迭代对象
    # 这里相当于一个join方法，直到tasks执行完之后，才会向下执行。
    print("last time :", time.time()-start_time)


1..
获取函数的返回值：
"""获取返回值"""
    loop = asyncio.get_event_loop()
    get_future = asyncio.ensure_future(get_html("http://baidu.com"))
    # 或者 等价于
    tasks = loop.create_task(get_html("http://baidu.com"))
    loop.run_until_complete(get_future)
    print(get_future.result())
    print(tasks.result())

为什么这里的结果是一样的呢？ get_future 命名没有Loop循环事件，他是怎么知道的loop呢，因为 在ensure_future()的内部，还是使用的 loop.creat_task()方法，如果不存在 循环事件 ，他就会自己创建一个 Loop，即loop = asyncio.get_event_loop()， 因此最终得到的效果是一样的。

但是 getfuture 是 tasks的子集

2..
  运行完之后，运行一个函数 call_back 之后，在执行return 语句。
 def call_back(url, future):
    print(url)
    print("调用此函数后，在执行return 语句")

 tasks.add_done_callback(partial(call_back, "http://xinhua.com"))
pritial（）是一个偏函数，就是将默认的参数集成化，返回函数名



3..wait 和 gather 的区别

等待 tasks 协程执行完之后，在继续执行下去， 参数和线程的wait()一样，wait的return_when参数可以是：FIRST_COMPLETED 完成一个后执行主线程、ALL_COMPLETED 完成所有的线程执行完在执行、或者其他的


gather 也能完成 wait 的功能，

loop.run_until_complete(asyncio.gather(*tasks))

只是要加一个 * 号，用以生成参数

但是 gather比 wait 更加高级：更加灵活，使用的时候最好用gather
 可以将任务分组，就可以分批次的取消或者运行

    group1 = [get_html("http://baidu.com") for i in range(5)]
    group2 = [get_html("http://baidu.com") for i in range(5)]
    loop.run_until_complete(asyncio.gather(*group1, *group2))

还能取消任务：
    group1 = [get_html("http://baidu.com") for i in range(5)]
    group2 = [get_html("http://baidu.com") for i in range(5)]
    group1 = asyncio.gather(*group1)
    group2 = asyncio.gather(*group2)
    group2.cancel()
    loop.run_until_complete(asyncio.gather(group1, group2))


13.3 task取消和子协程调用原理

 run_until_complete  和 run_forever 的区别

run_until_complete 中有一个 stop方法，用以停止loop ，应用比run_forever比较多

loop 会被放到future中去

取消future（task）
task 任务的取消：
  task.cancel()

async def get_html(sleep_times):
    print("waiting")
    await asyncio.sleep(sleep_times)
    print("end done time {}".format(sleep_times))


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    tasks = [get_html(2), get_html(3), get_html(3)]

    # 使用外键 ctrl+c 终止 任务
    try:
        loop.run_until_complete(asyncio.wait(tasks))
    except KeyboardInterrupt:
        all_tasks = asyncio.Task.all_tasks()
        for task in all_tasks:
            print("cancel task")
            print(task.cancel())
        loop.stop()
        loop.run_forever()
    finally:
        loop.close()  # close比stop 的内容更多。要清空很多东西


子协程调用原理

其实这个原理就和 之前的 select+回调函数+事件循环  差不多。

例子：

EventLoop -> Task ->print_sum() [相当于委托生成器]-> compute() ->Task ->EventLoop
EventLoop ->Task ->compute() -> print_sum() ->Task ->EventLoop

"""子协程调用原理"""
import asyncio


async def compute(x, y):
    print("compute {} + {}:".format(x, y))
    await asyncio.sleep(1)
    return x+y


async def print_sum(x, y):
    result = await compute(x, y)
    print("result is : {}".format(result))


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    loop.run_until_complete(print_sum(1, 2))
    loop.close()
   

13.4 call_soon， call_later, call_at , call_soon_threadsafe

call_soon()立即执行
call_later  延迟指定时间后运行后执行
call _at  在指定时间 now +1 后运行
call_soon_threadsafe  线程安全的立即运行

import asyncio


def call_back(sleep_times, loop):
    print("success time {} at {}".format(sleep_times, loop.time()))


def stop_loop(loop):
    loop.stop()


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    now = loop.time()

    # call_soon()立即执行
    loop.call_soon(call_back, 4, loop)
    # call_later  延迟指定时间后运行后执行
    loop.call_later(3, call_back, 3, loop)
    # call _at  在指定时间 now +1 后运行
    loop.call_at(now+1, call_back, 2, loop)
    loop.call_at(now+2, call_back, 2, loop)
    loop.call_at(now+3, call_back, 2, loop)
    # call_soon_threadsafe  线程安全的立即运行
    loop.call_soon_threadsafe(call_back, 5, loop)
    # 停止 run_forerver 运行
    # loop.call_soon(stop_loop, loop)
    # 循环一直运行
    loop.run_forever()



13.5 ThreadPoolExecutor 和asyncio 完成阻塞 IO 请求

# asyncio 既可以完成多线程，多进程，和 协程

# 多线程使用：
# 协程中强行使用阻塞io（如果某个库，或者某个接口只能提供阻塞的)， 使用多线程就好

import asyncio ,time
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
import socket


def get_url(url):
    # 通过socket请求html
    url = urlparse(url)
    host = url.netloc
    path = url.path
    if path == "":
        path = "/"

    # 建立socket连接
    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client.connect((host, 80))  # 这是阻塞的，阻塞不会消耗cpu，但会浪费cpu资源

    client.send("GET {} HTTP/1.1\r\nHost:{}\r\nConnection:close\r\n\r\n".format(path, host).encode("utf8"))

    data = b""
    while True:
        d = client.recv(1024)
        if d:
            data += d
        else:
            break
    data = data.decode("utf8")
    # 为了去掉 head部分
    html_data = data.split("\r\n\r\n")[1]
    print(html_data)
    client.close()


if __name__ == "__main__":
    start_time = time.time()
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(2)
    tasks = []
    for i in range(10):
        url = "http://baidu.com/{}".format(i)
        task = loop.run_in_executor(executor, get_url, url)
        tasks.append(task)
    loop.run_until_complete(asyncio.wait(tasks))
    print("last time: {}".format(time.time() - start_time))




13.6 asyncio模拟 http请求


"""使用asyncio模拟 http请求"""

import asyncio
import time
from urllib.parse import urlparse


async def get_url(url):
    # 通过socket请求html
    url = urlparse(url)
    host = url.netloc
    path = url.path
    if path == "":
        path = "/"

    # 建立socket连接
    reader, writer = await asyncio.open_connection(host, 80)
    writer.write("GET {} HTTP/1.1\r\nHost:{}\r\nConnection:close\r\n\r\n".format(path, host).encode("utf8"))
    all_lines = []
    async for raw_line in reader:
        data = raw_line.decode("utf8")
        all_lines.append(data)
    html = "\n".join(all_lines)
    return html


async def main():
    """ 在外部打印结果，执行一个打印一个"""
    tasks = []
    for i in range(10):
        url = "http://baidu.com"
        tasks.append(asyncio.ensure_future(get_url(url)))
    for task in asyncio.as_completed(tasks):
        result = await task
        print(result)


if __name__ == "__main__":
    start_time = time.time()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
    # 在外部打印
    # tasks = []
    # for i in range(10):
    #     url = "http://baidu.com"
    #     tasks.append(asyncio.ensure_future(get_url(url)))
    # loop.run_until_complete(asyncio.wait(tasks)
    # for task in tasks:
    #     print(result)
    print("last time: {}".format(time.time() - start_time))



13.7 future  和 task

 future 就是一个结果容器，当call_back运行完之后，就放在里面。

 task 是 future 的子类 ，是 协程 和 future之间的一个桥梁

 1： 激活协程
 2 ： 将协程做完之后，抛出的 StopIteration 做了处理

协程future 是为了保持和线程的future的逻辑和接口保持一致，因此，加入了 Task这个子类用来干一些和线程中的不一样的东西



13.8 asyncio 的同步和通信

协程同步：

"""
协程同步 Lock
不能用阻塞的
必须用 asyncio 的lock，而不是系统提供的 Lock，不需要，这是一个单线程。
里面的acquire()方法，采用yield from ，不是一个阻塞的方法。
当 parse_stuff 和 use_stuff 同时向get_stuff请求的话，而缓存 cache中又没有数据，
就容易被反爬
"""

import asyncio
from asyncio import Queue,Lock

lock = Lock()
cache = {}


async def get_stuff(url):
    # 或者  with await lock
    # 或者 开始 lock.acquire()  结尾：lock.release()
    async with lock:  # (这里调用的是魔法函数： __await__和 __aenter__)
        if url in cache:
            return cache[url]
        stuff = await aiohttp.request("GET", url)
        cache[url] = stuff
        return stuff


async def parse_stuff(url):
    stuff = await get_stuff(url)


async def use_stuff(url):
    stuff = await get_stuff(url)

if __name__ == "__main__":
    tasks = [parse_stuff(), use_stuff()]
    loop= asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))


协程的通信：
from asyncio import Queue

Quqe 里面有一个最大长度，可以达到限流的目的。如果只是为了实现通信的话，直接可以实现一个 全局的变量 queue =[] 也可以

queue的使用和线程一样，只是：
queueget和put方法，是一个协程，需要加await
await.queue.get()
await.queue.put()



13.9 aiohttp 实现高并发爬虫

 sanic 服务器： 高并发高性能的服务器


"""
使用Python3.7 + aiohttp +aiomysql +pyquery
 实现高并发的爬虫
 高并发的web服务器，除了aiohttp，还可以使用 sanic, 次服务器的性能号称可以和go语言媲美
 详见 github

 aiohttp : 搭建 HTTP client 和 server， 解析url
 aiomysql： 有三种方式： 此处选用 Connection pool
 pyquery ： 解析网页html
 需求： 爬取 伯乐在线 这个网站
       asyncio爬虫，去重，入库。
"""

import asyncio
import re

import aiomysql
import aiohttp
from pyquery import PyQuery


start_url = "http://www.jobbole.com/"
# 待爬取的url
waitting_urls = []
# 爬取过的url
seen_urls = set()      # 如果太大，不应使用，太耗内存， 应用布隆过滤器
stopping = False

sem = asyncio.Semaphore(3)  # 并发限制数量为3个


async def fetch(url, session):
    """解析传递过来的url 的html"""
    async with sem:
        await asyncio.sleep(1)  # 模拟并发一秒钟3个
        try:
            async with session.get(url) as resp:
                print("url status:{}".format(resp.status))
                if resp.status in [200, 201]:
                    data = await resp.text()
                    return data
        except Exception as e:
            print(e)


def extract_url(html):
    """解析html中的url"""
    urls = []
    pq = PyQuery(html)
    for link in pq.items("a"):  # <a> 标签
        url = link.attr("href")
        if url and url.startswith("http") and url not in seen_urls:
            urls.append(url)
            waitting_urls.append(url)
    return urls


async def init_urls(url, session):
    """ 解析start_url的 html"""
    """解析非详情页的url的html,并将该html 拿去再次解析出包含的url"""
    html = await fetch(start_url, session)
    seen_urls.add(url)
    extract_url(html)


async def article_handler(url, session, pool):
    """获取url的html， 并将指定的内容入库"""
    """获取文章详情，并解析入库"""
    html = await fetch(url, session)
    seen_urls.add(url)
    extract_url(html)
    pq = PyQuery(html)
    title = pq("title").text()
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            await cur.execute("SELECT 42;")
            insert_sql = "insert into article_test(title) values('{}')".format(title)
            await cur.execute(insert_sql)


async def consumer(pool):
    """在 waitting_urls中取出url 找到指定的东西，并入库"""
    async with aiohttp.ClientSession() as session:
        while not stopping:
            if len(waitting_urls) == 0:  # 判断 是否有等待的url
                await asyncio.sleep(1)
                continue
            url = waitting_urls.pop()
            print("start get url :{}".format(url))
            if re.match('http://.*?jobbole.com/\d+/', url):
                if url not in seen_urls:
                    await asyncio.ensure_future(article_handler(url, session, pool))
                    await asyncio.sleep(2)
            else:
                if url not in seen_urls:
                    await asyncio.ensure_future(init_urls(url, session))


async def main(loop):
    # 等待 mysql 连接建立好
    pool = await aiomysql.create_pool(host='127.0.0.1', port=3306,
                                           user='root', password='lixing798545',
                                           db='aiohttp_test', loop=loop, charset='utf8', autocommit=True)
    async with aiohttp.ClientSession() as session:
        html = await fetch(start_url, session)
        seen_urls.add(start_url)
        extract_url(html)
    # await asyncio.ensure_future(init_urls(start_url, session))
    await asyncio.ensure_future(consumer(pool))


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    asyncio.ensure_future(main(loop))
    loop.run_forever()
